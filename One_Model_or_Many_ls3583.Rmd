---
title: "One Model or Many?"
author: "Li Su, ls3583"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55))
```

```{r seed}
set.seed(35)
```

```{r libraries}
library(class)
library(glmnet)
library(rpart)
library(caret)
library(randomForest)
library(gbm)
library(e1071)
library(data.table)
library(DT)
library(tidyverse)
```

```{r source_files}

```

```{r constants}

```


```{r functions}

```


```{r load_data}
data <-read.csv('Diabetes and Readmissions.csv')
summary(data$readmitted)
```

```{r clean_data}
#recode response variable and inspect it using summary():
data$readmitted  <- ifelse(data$readmitted == "<30", TRUE, FALSE)
summary(data)

##The following classes were imbalanced, and removed:
#max_glu_serum ,A1Cresult,repaglinide ,  metformin, nateglinide,     chlorpropamide, glipizide, glimepiride ,glyburide ,      pioglitazone ,   rosiglitazone ,   acarbose  
#Also removed diag_1, diag_2, and diag_3 variables that have too many levels
#remove encounter_id, patient_nbr 
#remove discharge_disposition as the test set has different level from train set

df <- data[,c(-1:-2,-13:-15,-17:-28,-34)]
names(df)
```

## Introduction

The following 18 variables will be used to build models:
 "race"               "gender"             "age"                "time_in_hospital"  
 "num_lab_procedures" "num_procedures"     "num_medications"    "number_outpatient" 
 "number_emergency"   "number_inpatient"   "number_diagnoses"   "insulin"           
 "change"             "diabetesMed"        "readmitted"         "admission_type"    
 "admission_source"   "evaluation_set" 
 
 This project will assess and compare the prediction performance between using different subgroups of a dataset and using a larger overall dataset. 
 Since it it a binary classfication problem to predict whether a patient will be readmitted to hospital or not, and the dataset contains many categorical variables, I create 5 classfifers: Logistic Regression, Penalized Logistic Regression - Lasso, Decision Trees, Random Forest Classifier and Support Vector Machines.

## Models {.tabset}

### Category 1:  Logistic Regression {.tabset}

#### One Model

```{r cat1_one_model}
train <- df[df$evaluation_set == 'train', -18]
test <- df[df$evaluation_set == 'test', -18]

fit.logit.one <- glm(formula = readmitted ~., data = train, family = binomial)

prob.logit.one <- predict(fit.logit.one, newdata = test, type = 'response')
pred.logit.one <- ifelse(prob.logit.one>0.5, TRUE, FALSE)

#evaluation: prediction accuracy
acc.logit.one <- mean(pred.logit.one == test$readmitted)
acc.logit.one #0.8891365
```

#### Many Models

```{r cat1_many_models}
#create train subset list by appending filtered outcome to a list
train.subset  <- list()
for (x in c('[0-50)',  '[50-60)',  '[60-70)',  '[70-80)','[80-100)' )){ 
  tmp <-  train[train$age == x,]
  train.subset <- rlist::list.append(train.subset, tmp)
}
#create test subset list by appending filtered outcome to a list
test.subset <- list()
for (x in c('[0-50)',  '[50-60)',  '[60-70)',  '[70-80)','[80-100)' )){ 
  tmp <-  test[test$age == x,]
  test.subset <- rlist::list.append(test.subset, tmp)
}


#iteration model - run through subgroup and output the predicted class
pred.logit.many <- function(){
  pred.list <- list()
  for (i in 1:5){
    #remove age column, column index3, from predictors
    logit.many <- glm(formula = readmitted ~ ., data = train.subset[[i]][,-3], family = binomial)
    #make predictions on the test data
    pred.list[[i]] <- data.frame(ifelse((predict(logit.many, newdata = test.subset[[i]][,-3], type = 'response'))>0.5, 1, 0 ))
  }
  pred.aggregate <- rbindlist(pred.list)
  return (pred.aggregate)
}

pred.logit.many <- pred.logit.many()

#calculate the aggregated classification accuracy
acc.logit.many <-  mean(pred.logit.many == test$readmitted)
acc.logit.many  # 0.8891365
```


### Category 2:  Lasso Regression {.tabset}

#### One Model

```{r cat2_one_model}
# Find the best lambda using cross-validation
x <- model.matrix(readmitted ~., train)[,-1]
y <- train$readmitted
cv.lasso <- cv.glmnet(x,y, alpha = 1,family = 'binomial')
lasso.one <- glmnet(x, y, alpha = 1, family = "binomial", lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- model.matrix(readmitted ~., test)[,-1]
prob.lasso.one <- predict(lasso.one, newx = x.test)
pred.lasso.one <- ifelse(prob.lasso.one > 0.5, 1, 0)

# Model accuracy
acc.lasso.one <- mean(pred.lasso.one == test$readmitted)
acc.lasso.one  #0.8890396
```

#### Many Models

```{r cat2_many_models}
#iteration model - run through subgroup and output the predicted class
pred.lasso.many <- function(){
  pred.list <- list()
  for (i in 1:5){
    #remove age column, column index3, from predictors
    x <- model.matrix(readmitted ~., train.subset[[i]][,-3])[,-1]
    y <- train.subset[[i]]$readmitted
    cv.lasso.many <- cv.glmnet(x,y, alpha = 1,family = 'binomial')
    lasso.many <- glmnet(x, y, alpha = 1, family = "binomial", lambda =cv.lasso.many$lambda.min)
    #make predictions on the test data
    x.test <- model.matrix(readmitted ~., test.subset[[i]][,-3])[,-1]
    pred.list[[i]] <- data.frame(ifelse((predict(lasso.many, newx = x.test)) >0.5,1,0))
  }
  pred.aggregate <- rbindlist(pred.list)
  return (pred.aggregate)
}

pred.lasso.many <- pred.lasso.many()

#calculate the aggregated classification accuracy
acc.lasso.many <-  mean(pred.lasso.many == test$readmitted)
acc.lasso.many #0.8889427
```


### Category 3:  Decision Tree {.tabset}

#### One Model

```{r cat3_one_model}

tree <- rpart(readmitted~., data=train, method = "class")
print(tree)
# We are seeing only the root node for this tree model, 
#is probabaly due to the fact that we have extremely imbalanced target classes: 
prop.table(table(train$readmitted))

#force rpart to grow the more complex tree: minisplit of 1, and minbucket of 2
tree2 <- rpart(readmitted ~., data=train,control=rpart.control(minsplit = 1, minbucket = 2, cp = 2e-05), method = "class")

printcp(tree2)
#the more complex tree has two problems:
#the cp at root node is already very small
# and the cross-validated error increases as the mtry increases
#so the best tree will be the root node

#postpruning
pruned.tree <- prune(tree2, cp = tree2$cptable[which.min(tree2$cptable[,"xerror"]),"CP"])
printcp(pruned.tree)
#the pruned tree is same to the root node tree

#make prediction
pred.tree <- predict(pruned.tree, test,type = 'class' )

#accuracy
acc.tree.one <- mean(pred.tree == test$readmitted)
acc.tree.one #0.8892335
```

#### Many Models

```{r cat3_many_models}
#iteration model - run through subgroup and output the predicted class
pred.tree.many <- function(){
  pred.list <- list()
  for (i in 1:5){
    #remove age column, column index3, from predictors
    tree.many <- rpart(readmitted~., data = train.subset[[i]][,-3], method = "class")
    pruned.tree.many <- prune(tree.many, cp = tree.many$cptable[which.min(tree.many$cptable[,"xerror"]),"CP"])
    #make predictions on the test data
    pred.list[[i]] <- data.frame(predict(pruned.tree.many, newdata = test.subset[[i]][,-3], type = 'class'))
  }
  pred.aggregate <- rbindlist(pred.list)
  return (pred.aggregate)
}

pred.tree.many <- pred.tree.many()

#calculate the aggregated classification accuracy
acc.tree.many <-  mean(pred.tree.many == test$readmitted)
acc.tree.many #0.8892335
```


### Category 4:  Random Forest {.tabset}

#### One Model

```{r cat4_one_model}
#run randomForest with 10 trees (for relative fast computation) and default mtry
rf.one <- randomForest(readmitted~.,data= train,ntree=10, response = 'class')

prob.rf.one <- predict(rf.one, newdata = test, type = 'class')
pred.rf.one <- ifelse(prob.rf.one>0.5,1,0)

acc.rf.one <- mean(pred.rf.one == test$readmitted)
acc.rf.one #0.8847757
```

#### Many Models

```{r cat4_many_models}
pred.rf.many <- function(){
  pred.list <- list()
  for (i in 1:5){
    #remove age column, column index3, from predictors
    rf.many <- randomForest(readmitted ~ ., data= train.subset[[i]][,-3], ntree=10, method = 'class')
#make predictions on the test data
    pred.list[[i]] <- data.frame(ifelse(predict(rf.many, newdata = test.subset[[i]][,-3], type = 'class') > 0.5, 1, 0))
  }
  pred.aggregate <- rbindlist(pred.list)
  return (pred.aggregate)
}

pred.rf.many <- pred.rf.many()

#calculate the aggregated classification accuracy
acc.rf.many <-  mean(pred.rf.many == test$readmitted)
acc.rf.many #0.8828375
```


### Category 5:  Support Vector Machines{.tabset}

#### One Model

```{r cat5_one_model}
svm.one <- svm(readmitted~., data = train, kernel = 'linear', type = "C-classification", cross= 5, cost = 0.01, gamma = 1000, scale = FALSE)
pred.svm <- predict(svm.one, newdata = test)

acc.svm.one <- mean(pred.svm == test$readmitted)
acc.svm.one #0.8892335
```

#### Many Models

```{r cat5_many_models}

pred.svm.many <- function(){
  pred.list <- list()
  for (i in 1:5){
    #remove age column, column index3, from predictors
    svm.many <- svm(readmitted ~., data= train.subset[[i]][,-3],kernel = 'linear', type = "C-classification", cross= 5, cost = 0.01, gamma = 1000, scale = FALSE)
    #make predictions on the test data
    pred.list[[i]] <- data.frame(predict(svm.many, newdata = test.subset[[i]][,-3]))
  }
  pred.aggregate <- rbindlist(pred.list)
  return (pred.aggregate)
}

pred.svm.many <- pred.svm.many()

#calculate the aggregated classification accuracy
acc.svm.many <- mean(pred.svm.many == test$readmitted)
acc.svm.many  #0.8892335
```

## Scoreboard

```{r scoreboard}
#combine the results to one datatable
a <- c('Category 1','Category 2','Category 3','Category 4','Category 5')
b <- c('Logistic Regression', 'Lasso', 'Tree','Random Forest','Support Vector Machine')
c <- rbind(acc.logit.one,acc.lasso.one,acc.tree.one,acc.rf.one,acc.svm.one)
d <- rbind(acc.logit.many, acc.lasso.many,acc.tree.many,acc.rf.many,acc.svm.many)

score.df <- data.frame(a, b, c,  d,  c-d)
datatable(score.df, rownames = FALSE, colnames = c('Category','Modeling Type','Proportion Classified, One Model','Proportion Correctly Classified, Many Models','Difference' ))

```

## Discussion

From the score table, we are seeing no significant differences for the prediction accuracy between one model and many models, only slight difference that can be round to 0 for Lasso Classifier and Random Forest Classifier. 

Within different categories of algothrims:

Supervised Learning methods are heavily affected by imbalanced data.Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.

We achieve exact same accuracy using simple algorithms like Logistic Regression, simple Decision Tree and simple SVM. Altough from score table our accuracy looks excellent (close to 90%), but the accuracy is only reflecting the underlying class distribution in the training set (FALSE: TRUE = 0.9:0.1). We can confirm that from decision tree model where the optimal tree is just the root node. Therefore, the classifiers just decide that the best thing to do is always predict FALSE and achive accuracy.

In penalized version such as Lasso/Penalized-logistic Regression, the classifier imposes and additional cost on the model for making classification mistakes on the minority class during training, therefore pay more attention to minority classes. The Lasso accuracy is slightly different to the simple logistic regression.
Random Forest classifier, on the other hand, takes Decision Tree further by creating multiple trees from different subsets of the training data. We also reach a slightly different outcome.

Within each categories of algothrims:

Because of similarly imbalanced classes (0.9:0.1) in each age subgroup, in many models, simple methods still choose to predict FALSE and achieve accuracy. That is why we see no difference between one model and many models.

Random Forest and Lasso/Penalized Logistic Regression produce different outcomes, since sample size matters more in these approach. 
Random Forest relies largely on sample size as it needs to create multiple trees from diffferent subsets of the training data. 
With small numbers of samples, the amount of penalization that we need to get a reliable model may be very great that we get limited predictive power. This explains the slight accuracy differences between one model and many models.


## References


https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/

https://github.com/LangilleLab/microbiome_helper/wiki/Random-Forest-in-R-with-Large-Sample-Sizes

