---
title: "Homework 4"
author: "Li Su, ls3583"
date: ""
output:
  prettydoc::html_pretty:
  theme: cayman
highlight: github
---
  
```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55), tidy = FALSE)
```

```{r seed}
set.seed(29923)
```

```{r libraries}
library(tidyverse)
library(data.table)
library(caret)
library(leaps)
<<<<<<< HEAD
library(glmnet)
=======
>>>>>>> refs/remotes/origin/master
```

```{r source_files}

```

```{r functions}

```

```{r constants}

```


```{r load_data}

```

```{r clean_data}


```


## About the Data

For this assignment, we will be analyzing data from users of Google Reviews.  The file **ratings.csv** contains (lightly edited) information on the average ratings of thousands of users across a wide variety of categories.  All of the user's ratings were on a scale from 0 to 5, and these values were averaged by category.  Each user's averages for the categories appear in one row of the file.  For more details, see http://archive.ics.uci.edu/ml/datasets/Tarvel+Review+Ratings#.

The data includes a variable called **user** that provides a unique identifier.  The **set** variable divided the data into training and testing sets.  Otherwise, all of the variables are categories of ratings.

Using these data, answer the following questions.

## Question 1:  Preparation and Summarization

### 1a:  Creating an Outcome

For this study, we will be focused on the question of predicting the ratings of **accommodations** for travelers in terms of all of the other experiences available.  Because travelers can either stay in **resorts** or in **hotels_lodging**, we will create an overall measure of satisfaction.  Add a column to your data set named **accommodations**.  This will be defined as the user's average of their scores on **resorts** and **hotels_lodging**.  Show the code for how you constructed the **accommodations** variable.

```{r 1a}
df <- read.csv('ratings.csv')
str(df)

df['accommodations'] <- (df['resorts']+df['hotels_lodging'])/2
head(df)
```

### 1b:  Summarization

For each category of rating, including the newly created **accommodations** variable, show the average and the standard deviation of the recorded values on the training set.  Show the results in a table.  Round your answers to a reasonable number of decimal places.

```{r q1}

category <- c('hotels_lodging','resorts','accommodations','churches','beaches','parks','theaters','museums','malls', 'zoo','restaurants','bars_pubs','local_services', 'burger_pizza' , 'juice_bars' ,'art_galleries','dance_clubs','swimming_pools', 'gyms' , 'bakeries' ,'beauty_spas' ,'cafes',  'view_points',  'monuments','gardens')

train <- df[df['set'] == 'train', category]
test <- df[df['set'] == 'test', category]

avg_sd <- function(x) {
  c(mean = round(mean(x, na.rm = TRUE),2), 
    sd = round(sd(x,na.rm = TRUE),2)
  )
}

sapply(train,FUN = avg_sd)
as.data.table(sapply(train,FUN = avg_sd), keep.rownames = "measure")

```

## Question 2:  Linear Regression

### 2a

Use the training data to create a linear regression model for the **accommodations** outcome.  The predictor variables should include every rating variable except for **resorts** and **hotels_lodging**.  No other predictors should be used.  Build the model and display a summary of the coefficients.  Show a summary of the resulting model's coefficients, rounded to a reasonable number of digits.

```{r 2a}
#exclude 'resorts', 'hotels_lodging' and 'set' from train and test sets.

train <- train[-c(1,2)]
test <- test[-c(1,2)]

<<<<<<< HEAD
fit.ols <-lm (accommodations ~.,data = train)

coef.ols <- round(fit.ols$coefficients,2)
coef.ols
=======
lm.mod <-lm (accommodations ~.,data = train)

coeff <- round(lm.mod$coefficients,2)
coeff
>>>>>>> refs/remotes/origin/master

```

### 2b

Based on the linear model's results, which categories are associated with an **increase** in the average ratings for **accommodations** in a statistically significant way?  Display the summary of the linear model's coefficients for this set of variables.  This table should be sorted in order of the effect size (the estimated coefficient) to show the strongest effects first.

```{r 2b}
#extract p.values of the lm model
<<<<<<< HEAD
p.values.ols <- summary(fit.ols)$coefficients[, 4]

#subset variables with 0.05 signigicance level
coef.select <- coef.ols[p.values.ols < 0.05]
coef.select
#select those whose coefficients are positive, and sort in descending 
sort(coef.select[coef.select>0], decreasing = TRUE)
=======
p.values <- summary(lm.mod)$coefficients[, 4]

#subset variables with 0.05 signigicance level
coeff.select <- coeff[p.value < 0.05]
coeff.select
#select those whose coefficients are positive, and sort in descending 
sort(coeff.select[coeff.select>0], decreasing = TRUE)
>>>>>>> refs/remotes/origin/master

```

### 2c

Which categories are associated with an **decrease** in the average ratings for **accommodations** in a statistically significant way?  Display the summary of the linear model's coefficients for this set of variables.  This table should be sorted in order of the effect size (the estimated coefficient) to show the strongest effects first.

```{r 2c}
<<<<<<< HEAD
sort(coef.select[coef.select< 0], decreasing = FALSE)
=======
sort(coeff.select[coeff.select< 0], decreasing = FALSE)
>>>>>>> refs/remotes/origin/master
```


### 2d

Based on the linear model's results, which categories did not show statistically significant relationships with the **accommodations**?

```{r 2d}
<<<<<<< HEAD
coef.unselect <- coef.ols[p.values.ols >= 0.05]
coef.unselect
=======
coeff.unselect <- coeff[p.values >= 0.05]
coeff.unselect
>>>>>>> refs/remotes/origin/master
```


### 2e

Using the root mean squared error (RMSE) as a metric, how accurate is the linear model in terms of predicting the ratings for **accommodations** on the testing set?


```{r 2e}

<<<<<<< HEAD
pred.ols <- predict(fit.ols, newdata = test)
rmse.ols <- sqrt(mean(pred.ols- test$accommodations)^2)
rmse.ols
=======
pred <- predict(lm.mod, newdata = test)
lm.rmse <- sqrt(mean(pred- test$accommodations)^2)
lm.rmse
>>>>>>> refs/remotes/origin/master

```

## Question 3:  Selection Procedures

### 3a

Use **forward stepwise regression** to create a separate linear regression model of **accommodations** on the training set.  The procedure should start with a model that only includes an intercept, and allowing the model to grow as large as including all of the predictors used in Question 2.  Show a summary of the resulting model's coefficients, rounded to a reasonable number of digits.

**Note**:  The **capture.output** function can be used to prevent R from printing out all of the intermediate calculations performed in stepwise regression.  You are not required to use this method, but it will help you to create reports that maintain good readability while using methods like this.

```{r 3a}

empty.mod = lm(accommodations~1,data=train)
full.mod = lm(accommodations~.,data=train)

output.forward <- capture.output(forwardStepwise <- step(empty.mod,scope=list(upper=full.mod,lower=empty.mod),direction='forward'))
<<<<<<< HEAD
=======
summary(forwardStepwise)
>>>>>>> refs/remotes/origin/master

round(forwardStepwise$coefficients,2)

```

### 3b

Use **backward stepwise regression** to create a separate linear regression model of **accommodations** on the training set.  The procedure should start with the full model you built in Question 2 while allowing the model to become as small as one that only includes an intercept.  Show a summary of the resulting model's coefficients, rounded to a reasonable number of digits.


```{r 3b}

output.backward <- capture.output(backwardStepwise <- step(full.mod, scope = list(upper = full.mod, lower = empty.mod), direction='backward'))

round(backwardStepwise$coefficients,2)

```

### 3c

Describe the similarities and differences in the results obtained by forward and backward stepwise selection.

Forward and backward stepwise selections are all greedy feature selection algorithms that only look at a step ahead to find the best subset. Both approaches search through only 1+p(p+1)/2 models, thus don't guarantee to yield the best subset among all possible subsets.They both use methods such as AIC, BIC or adjusted R^2 to determine the single best model.

Forward stepwise selection starts from an empty model to the full model, with each step adding a predictor to the model, and backward stepwise selection does the opposite - from full model to empty model.
To fit the backward stepwise algorithm, sample size n should be larger than the number of predictors p. In contrast, forward stepwise can be used even when n<p, and so is the only viable subset method when p is very large. 


### 3d

Use the results from the forward selection and backward selection models to make predictions on the testing set.  Calculate the RMSE of each set of predictions.  Show the RMSE of linear regression, forward selection, and backward selection in a table.  Round the results to a reasonable number of digits.

```{r 3d}

<<<<<<< HEAD
pred.forward <- predict(forwardStepwise, newdata=test)
pred.backward <- predict(backwardStepwise, newdata=test)

rmse.forward <- sqrt(mean(pred.forward - test$accommodations)^2)
rmse.backward <- sqrt(mean(pred.backward - test$accommodations)^2)

round(data.table(rmse.ols, rmse.forward, rmse.backward),4)

=======
forward.pred <- predict(forwardStepwise, newdata=test)
backward.pred <- predict(backwardStepwise, newdata=test)

forward.rmse <- sqrt(mean(forward.pred - test$accommodations)^2)
backward.rmse <- sqrt(mean(backward.pred - test$accommodations)^2)

data.table(lm.rmse, forward.rmse, backward.rmse)
>>>>>>> refs/remotes/origin/master
```


## Question 4:  Regularized Regression

### 4a

Use **ridge regression** to create a model of **accommodations** on the training set.  The model should include the same predictors used to build the linear regression above. Display the model's coefficients, rounded to a reasonable number of digits.

This can be implemented using the **glmnet** function in the **glmnet** package.  Note that ridge regression is specified when **alpha = 0**.

```{r 4a}

<<<<<<< HEAD
x<- data.matrix(train[, 2:23])
y <- train$accommodations
fit.ridge <- cv.glmnet(x, y, alpha = 0 )
round(coef(fit.ridge),2)

=======
>>>>>>> refs/remotes/origin/master
```

### 4b

Use **lasso regression** to create a model of **accommodations** on the training set.  The model should include the same predictors used to build the linear regression above. Display the model's coefficients, rounded to a reasonable number of digits.

This can be implemented using the **glmnet** function in the **glmnet** package.  Note that lasso regression is specified when **alpha = 1**.

```{r 4b}

<<<<<<< HEAD
fit.lasso <- cv.glmnet(x, y, alpha = 1 )
round(coef(fit.lasso),2)

=======
>>>>>>> refs/remotes/origin/master
```

### 4c

Use the ridge and lasso regression models to generate predictions on the testing set.  Compute the RMSE for each set of predictions.  Add these values to the table of RMSE values that includes those for linear regression and the stepwise procedures.  Round the table to a reasonable number of digits.

```{r 4c}

<<<<<<< HEAD
pred.lasso <- predict(fit.lasso, data.matrix(test[,2:23]))
pred.ridge <- predict(fit.ridge, data.matrix(test[,2:23]))
rmse.lasso <- sqrt(mean(pred.lasso - test$accommodations)^2)
rmse.ridge <- sqrt(mean(pred.ridge - test$accommodations)^2)

round(data.table(rmse.ols, rmse.forward, rmse.backward, rmse.ridge, rmse.lasso),4)

=======
>>>>>>> refs/remotes/origin/master
```

### 4d

Comment on the results.  Were the results of the models reasonably similar or quite different?  What is the reason for this?

<<<<<<< HEAD
The results of the models reasonably similar to each other mainly because the number of observations is much much larger than the number of features in the dataset. Least squares regression technique is intended for low-dimensional setting like this. The resulting linear model can perform reasonabaly well on the test set.

=======
>>>>>>> refs/remotes/origin/master

## Question 5

How would the results for the regularized methods (ridge and lasso) have changed if we had utilized less data in the training set?  We will explore this question in the following parts.

### 5a

Create a reduced training set that only contains the first 250 rows of the training data.  Then fit a ridge regression model on this reduced training set with a similar specification to the earlier model.  Display the coefficients of the model, rounded to a reasonable number of decimal places.

```{r 5a}
<<<<<<< HEAD
dim(train)
train.reduced <- train[1:250,]

fit.ridge1 <- cv.glmnet(data.matrix(train.reduced[, 2:23]), train.reduced$accommodations, alpha = 0 )
round(coef(fit.ridge1),2)
=======

>>>>>>> refs/remotes/origin/master

```


### 5b

Now fit a lasso regression model on this reduced training set with a similar specification to the earlier model.  Display the coefficients of the model, rounded to a reasonable number of decimal places.

```{r 5b}

<<<<<<< HEAD
fit.lasso1 <- cv.glmnet(data.matrix(train.reduced[, 2:23]), train.reduced$accommodations, alpha = 1 )
round(coef(fit.lasso1),2)

=======
>>>>>>> refs/remotes/origin/master
```

### 5c

How different are the coefficients for the full and reduced ridge regression models?

```{r 5c}

<<<<<<< HEAD
cbind(round(coef(fit.ridge),2),  round(coef(fit.ridge1),2))

=======
>>>>>>> refs/remotes/origin/master
```


### 5d

How different are the coefficients for the full and reduced lasso regression models?

```{r 5d}

<<<<<<< HEAD
cbind(round(coef(fit.lasso),2), round(coef(fit.lasso1),2))

```


=======
```



>>>>>>> refs/remotes/origin/master
### 5e

Use the ridge and lasso regression models that were fit on the **reduced** training set to generate predictions on the **full** testing set.  Compute the RMSE for each set of predictions.  Add these values to the table of RMSE values that include all of the earlier RMSE results.  Round the table to a reasonable number of digits.

```{r 5e}

<<<<<<< HEAD
pred.ridge1 <- predict(fit.ridge1, data.matrix(test[,2:23]))
pred.lasso1 <- predict(fit.lasso1, data.matrix(test[,2:23]))

rmse.ridge1 <- sqrt(mean(pred.ridge1 - test$accommodations)^2)
rmse.lasso1 <- sqrt(mean(pred.lasso1 - test$accommodations)^2)

round(data.table(rmse.ols, rmse.forward, rmse.backward, rmse.ridge, rmse.lasso, rmse.ridge1, rmse.lasso1),4)

=======
>>>>>>> refs/remotes/origin/master
```

### 5f

What conclusions can you draw about the usage of selection procedures and regularization methods based upon this work?

<<<<<<< HEAD
Subset selection procedures increase model interpretability by automatically performing feature selection in a multiple regression model. To fit the backward stepwise method, number of observations n should be larger than the number of predictors p. In contrast, forward stepwise can be used even when n < p.
Regulization methods can yeild better prediction accuracy compared to least squares in certain settings.If number of observations is much larger than p, the number of variables, then the least squares estimate tend to also have low variance, and hence will perform well on test observations. However, if n is not much larger than p, as we saw in the reduced train set, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on test set. If this case the least squares method will not be a good choice since the variance is infinite, while the regularization methods, by reducing the variance, perform well on unseen test set.
 
=======
>>>>>>> refs/remotes/origin/master
